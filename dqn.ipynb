{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catch import CatchEnv\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (torch.tensor(np.array(states), dtype=torch.float32).to(self.device),\n",
    "                torch.tensor(actions).to(self.device),\n",
    "                torch.tensor(rewards).to(self.device),\n",
    "                torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device),\n",
    "                torch.tensor(dones).to(self.device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self.feature_size(input_shape), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                state_shape,\n",
    "                num_actions,\n",
    "                device,\n",
    "                gamma,\n",
    "                epsilon,\n",
    "                epsilon_decay,\n",
    "                epsilon_min,\n",
    "                update_frequency,\n",
    "                learning_rate,\n",
    "                replay_buffer_size):\n",
    "\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        self.policy_net = DQN(state_shape, num_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, num_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=replay_buffer_size, device=self.device)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.update_frequency = update_frequency\n",
    "        self.timestep = 0\n",
    "\n",
    "    def select_action(self, state, test):\n",
    "        if not test and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        if len(self.replay_buffer) < 1000:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device).long()\n",
    "\n",
    "        q_values = self.policy_net(states)\n",
    "        next_q_values = self.policy_net(next_states)\n",
    "        target_q_values = self.target_net(next_states)\n",
    "        next_q_values_argmax = torch.argmax(next_q_values, dim=1)\n",
    "        next_q_value_target = target_q_values.gather(1, next_q_values_argmax.unsqueeze(1)).squeeze(1)\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_value_target\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        if self.timestep % self.update_frequency == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To match torch channel first format\n",
    "def process_state(state):\n",
    "    return state.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CatchEnv()\n",
    "state_shape = env.state_shape()\n",
    "num_actions = env.get_num_actions()\n",
    "\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "\n",
    "params = {\"state_shape\":state_shape, \"num_actions\":num_actions,\n",
    "          \"device\":device, \"gamma\":0.99, \"epsilon\":1.0, \"epsilon_decay\":0.00025,\n",
    "          \"epsilon_min\":0.01, \"update_frequency\":600,\n",
    "          \"learning_rate\":0.0001, \"replay_buffer_size\":10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(agent, batch_size, num_episodes):\n",
    "    train_episodes = 10\n",
    "    test_episodes = 10\n",
    "    avg_test_rewards = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Train\n",
    "        if episode % (train_episodes + test_episodes) < train_episodes:\n",
    "            state = env.reset()\n",
    "            state = process_state(state)\n",
    "            done = False\n",
    "            while not done: \n",
    "                action = agent.select_action(state, test=False)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_state = process_state(next_state)\n",
    "                agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                agent.update(batch_size)\n",
    "        # Test\n",
    "        else:\n",
    "            state = env.reset()\n",
    "            state = process_state(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = agent.select_action(state, test=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_state = process_state(next_state)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            test_rewards.append(total_reward)\n",
    "            if episode % 10 == 0:\n",
    "                avg_test_rewards.append(np.mean(test_rewards[-10:]))\n",
    "                print(f\"({episode}) Last 10 episodes mean test reward:\", avg_test_rewards[-1],\n",
    "                       \"Eps:\", round(agent.epsilon, 2))\n",
    "\n",
    "    print(\"Training finished after\", agent.timestep, \"timesteps.\")\n",
    "    return avg_test_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10) Last 10 episodes mean test reward: 0.0 Eps: 1.0\n",
      "(30) Last 10 episodes mean test reward: 0.3 Eps: 1.0\n",
      "(50) Last 10 episodes mean test reward: 0.1 Eps: 1.0\n",
      "(70) Last 10 episodes mean test reward: 0.0 Eps: 1.0\n",
      "(90) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(110) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(130) Last 10 episodes mean test reward: 0.3 Eps: 1.0\n",
      "(150) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(170) Last 10 episodes mean test reward: 0.1 Eps: 1.0\n",
      "(190) Last 10 episodes mean test reward: 0.2 Eps: 0.97\n",
      "(210) Last 10 episodes mean test reward: 0.3 Eps: 0.95\n",
      "(230) Last 10 episodes mean test reward: 0.7 Eps: 0.92\n",
      "(250) Last 10 episodes mean test reward: 0.3 Eps: 0.89\n",
      "(270) Last 10 episodes mean test reward: 0.1 Eps: 0.86\n",
      "(290) Last 10 episodes mean test reward: 0.4 Eps: 0.84\n",
      "(310) Last 10 episodes mean test reward: 0.2 Eps: 0.81\n",
      "(330) Last 10 episodes mean test reward: 0.6 Eps: 0.78\n",
      "(350) Last 10 episodes mean test reward: 0.3 Eps: 0.75\n",
      "(370) Last 10 episodes mean test reward: 0.4 Eps: 0.73\n",
      "(390) Last 10 episodes mean test reward: 0.5 Eps: 0.7\n",
      "(410) Last 10 episodes mean test reward: 0.2 Eps: 0.67\n",
      "(430) Last 10 episodes mean test reward: 0.2 Eps: 0.64\n",
      "(450) Last 10 episodes mean test reward: 0.3 Eps: 0.62\n",
      "(470) Last 10 episodes mean test reward: 0.4 Eps: 0.59\n",
      "(490) Last 10 episodes mean test reward: 0.3 Eps: 0.56\n",
      "(510) Last 10 episodes mean test reward: 0.2 Eps: 0.53\n",
      "(530) Last 10 episodes mean test reward: 0.6 Eps: 0.51\n",
      "(550) Last 10 episodes mean test reward: 0.2 Eps: 0.48\n",
      "(570) Last 10 episodes mean test reward: 0.4 Eps: 0.45\n",
      "(590) Last 10 episodes mean test reward: 0.5 Eps: 0.42\n",
      "(610) Last 10 episodes mean test reward: 0.3 Eps: 0.4\n",
      "(630) Last 10 episodes mean test reward: 0.4 Eps: 0.37\n",
      "(650) Last 10 episodes mean test reward: 0.1 Eps: 0.34\n",
      "(670) Last 10 episodes mean test reward: 0.3 Eps: 0.31\n",
      "(690) Last 10 episodes mean test reward: 0.6 Eps: 0.29\n",
      "(710) Last 10 episodes mean test reward: 0.4 Eps: 0.26\n",
      "(730) Last 10 episodes mean test reward: 0.2 Eps: 0.23\n",
      "(750) Last 10 episodes mean test reward: 0.6 Eps: 0.2\n",
      "(770) Last 10 episodes mean test reward: 0.4 Eps: 0.18\n",
      "(790) Last 10 episodes mean test reward: 0.4 Eps: 0.15\n",
      "(810) Last 10 episodes mean test reward: 0.5 Eps: 0.12\n",
      "(830) Last 10 episodes mean test reward: 0.3 Eps: 0.09\n",
      "(850) Last 10 episodes mean test reward: 0.6 Eps: 0.07\n",
      "(870) Last 10 episodes mean test reward: 0.4 Eps: 0.04\n",
      "(890) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(910) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(930) Last 10 episodes mean test reward: 0.4 Eps: 0.01\n",
      "(950) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(970) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(990) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1010) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1030) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1050) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1070) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1090) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1110) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1130) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1150) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1170) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1190) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1210) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1230) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1250) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1270) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1290) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1310) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1330) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1350) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1370) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1390) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1410) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1430) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1450) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1470) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1490) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1510) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1530) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1550) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1570) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1590) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1610) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1630) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1650) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1670) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1690) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1710) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1730) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1750) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1770) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1790) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1810) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1830) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1850) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1870) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1890) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1910) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1930) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1950) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1970) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1990) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2010) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2030) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(2050) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2070) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2090) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2110) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2130) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2150) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2170) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2190) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2210) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2230) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2250) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2270) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2290) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2310) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2330) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2350) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2370) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2390) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2410) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2430) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2450) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2470) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2490) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2510) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2530) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2550) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2570) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2590) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2610) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2630) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2650) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2670) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2690) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2710) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2730) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2750) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2770) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2790) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2810) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2830) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2850) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2870) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2890) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2910) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2930) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2950) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2970) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2990) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3010) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3030) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3050) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3070) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3090) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3110) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3130) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3150) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3170) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3190) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3210) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3230) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(3250) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3270) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3290) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3310) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3330) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3350) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3370) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3390) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3410) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3430) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3450) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3470) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3490) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(3510) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3530) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3550) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3570) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3590) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(3610) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3630) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3650) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3670) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3690) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3710) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3730) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3750) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3770) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3790) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3810) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3830) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3850) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3870) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3890) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3910) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3930) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3950) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3970) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3990) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "Training finished after 21001 timesteps.\n",
      "(10) Last 10 episodes mean test reward: 0.0 Eps: 1.0\n",
      "(30) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(50) Last 10 episodes mean test reward: 0.1 Eps: 1.0\n",
      "(70) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(90) Last 10 episodes mean test reward: 0.1 Eps: 1.0\n",
      "(110) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(130) Last 10 episodes mean test reward: 0.2 Eps: 1.0\n",
      "(150) Last 10 episodes mean test reward: 0.0 Eps: 1.0\n",
      "(170) Last 10 episodes mean test reward: 0.4 Eps: 1.0\n",
      "(190) Last 10 episodes mean test reward: 0.2 Eps: 0.97\n",
      "(210) Last 10 episodes mean test reward: 0.3 Eps: 0.95\n",
      "(230) Last 10 episodes mean test reward: 0.2 Eps: 0.92\n",
      "(250) Last 10 episodes mean test reward: 0.3 Eps: 0.89\n",
      "(270) Last 10 episodes mean test reward: 0.5 Eps: 0.86\n",
      "(290) Last 10 episodes mean test reward: 0.1 Eps: 0.84\n",
      "(310) Last 10 episodes mean test reward: 0.4 Eps: 0.81\n",
      "(330) Last 10 episodes mean test reward: 0.2 Eps: 0.78\n",
      "(350) Last 10 episodes mean test reward: 0.3 Eps: 0.75\n",
      "(370) Last 10 episodes mean test reward: 0.6 Eps: 0.73\n",
      "(390) Last 10 episodes mean test reward: 0.3 Eps: 0.7\n",
      "(410) Last 10 episodes mean test reward: 0.4 Eps: 0.67\n",
      "(430) Last 10 episodes mean test reward: 0.1 Eps: 0.64\n",
      "(450) Last 10 episodes mean test reward: 0.2 Eps: 0.62\n",
      "(470) Last 10 episodes mean test reward: 0.2 Eps: 0.59\n",
      "(490) Last 10 episodes mean test reward: 0.4 Eps: 0.56\n",
      "(510) Last 10 episodes mean test reward: 0.3 Eps: 0.53\n",
      "(530) Last 10 episodes mean test reward: 0.4 Eps: 0.51\n",
      "(550) Last 10 episodes mean test reward: 0.4 Eps: 0.48\n",
      "(570) Last 10 episodes mean test reward: 0.5 Eps: 0.45\n",
      "(590) Last 10 episodes mean test reward: 0.7 Eps: 0.42\n",
      "(610) Last 10 episodes mean test reward: 0.4 Eps: 0.4\n",
      "(630) Last 10 episodes mean test reward: 0.3 Eps: 0.37\n",
      "(650) Last 10 episodes mean test reward: 0.4 Eps: 0.34\n",
      "(670) Last 10 episodes mean test reward: 0.4 Eps: 0.31\n",
      "(690) Last 10 episodes mean test reward: 0.7 Eps: 0.29\n",
      "(710) Last 10 episodes mean test reward: 0.7 Eps: 0.26\n",
      "(730) Last 10 episodes mean test reward: 0.5 Eps: 0.23\n",
      "(750) Last 10 episodes mean test reward: 0.4 Eps: 0.2\n",
      "(770) Last 10 episodes mean test reward: 0.5 Eps: 0.18\n",
      "(790) Last 10 episodes mean test reward: 0.9 Eps: 0.15\n",
      "(810) Last 10 episodes mean test reward: 0.6 Eps: 0.12\n",
      "(830) Last 10 episodes mean test reward: 0.7 Eps: 0.09\n",
      "(850) Last 10 episodes mean test reward: 0.7 Eps: 0.07\n",
      "(870) Last 10 episodes mean test reward: 0.5 Eps: 0.04\n",
      "(890) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(910) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(930) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(950) Last 10 episodes mean test reward: 0.3 Eps: 0.01\n",
      "(970) Last 10 episodes mean test reward: 0.4 Eps: 0.01\n",
      "(990) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1010) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1030) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1050) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1070) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1090) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1110) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1130) Last 10 episodes mean test reward: 0.6 Eps: 0.01\n",
      "(1150) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1170) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1190) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1210) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1230) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1250) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1270) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1290) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1310) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1330) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1350) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1370) Last 10 episodes mean test reward: 0.5 Eps: 0.01\n",
      "(1390) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1410) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1430) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1450) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1470) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1490) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1510) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1530) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1550) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(1570) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1590) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1610) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1630) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1650) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1670) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1690) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1710) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(1730) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1750) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1770) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1790) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1810) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1830) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1850) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1870) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1890) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1910) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1930) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(1950) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1970) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(1990) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2010) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2030) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2050) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2070) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2090) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2110) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2130) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2150) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2170) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2190) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2210) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2230) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2250) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2270) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2290) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2310) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2330) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2350) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2370) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2390) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2410) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2430) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2450) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2470) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2490) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2510) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2530) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2550) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2570) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2590) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2610) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2630) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2650) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2670) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2690) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2710) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(2730) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2750) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2770) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2790) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2810) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(2830) Last 10 episodes mean test reward: 0.7 Eps: 0.01\n",
      "(2850) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2870) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2890) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2910) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2930) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(2950) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2970) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(2990) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3010) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3030) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3050) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3070) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3090) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3110) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3130) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3150) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3170) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3190) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3210) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3230) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3250) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3270) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3290) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3310) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3330) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3350) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3370) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3390) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3410) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3430) Last 10 episodes mean test reward: 0.8 Eps: 0.01\n",
      "(3450) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3470) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3490) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3510) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3530) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3550) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3570) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n",
      "(3590) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3610) Last 10 episodes mean test reward: 1.0 Eps: 0.01\n",
      "(3630) Last 10 episodes mean test reward: 0.9 Eps: 0.01\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(5):\n",
    "    set_seed(i)\n",
    "    agent = DQNAgent(**params)\n",
    "    avg_test_rewards = train_loop(agent, batch_size, num_episodes)\n",
    "    # np.save(f\"results/group_44_catch_rewards_{i+1}.npy\", avg_test_rewards)\n",
    "    results.append(avg_test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).melt()\n",
    "ax = sns.lineplot(x=\"variable\", y=\"value\", data=df)\n",
    "ax.set(xlabel='Number of test iteration (10 episodes each)',\n",
    "       ylabel='Average 10 episode reward',\n",
    "       title='Average test reward')\n",
    "ax.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
